{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#template base: https://medium.com/@dhiraj.p.rai/logistic-regression-in-spark-ml-8a95b5f5434c\n",
    "# Inicialização da sessão\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Telecom-CustomeChurn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carregando dataset - apesar de ter disponibilizado teste separado, farei o split da treino para simplificar o experimento\n",
    "dados = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(r\"projeto4_telecom_treino.csv\")\n",
    "dados.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#análise exploratória\n",
    "dados.describe().select('_c0',\n",
    " 'state',\n",
    " 'account_length',\n",
    " 'area_code',\n",
    " 'international_plan',\n",
    " 'voice_mail_plan',\n",
    " 'number_vmail_messages',\n",
    " 'total_day_minutes',\n",
    " 'total_day_calls',\n",
    " 'total_day_charge',\n",
    " 'total_eve_minutes',\n",
    " 'total_eve_calls',\n",
    " 'total_eve_charge',\n",
    " 'total_night_minutes',\n",
    " 'total_night_calls',\n",
    " 'total_night_charge',\n",
    " 'total_intl_minutes',\n",
    " 'total_intl_calls',\n",
    " 'total_intl_charge',\n",
    " 'number_customer_service_calls',\n",
    " 'churn').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#análise exploratória - campo Summary é parâmetro estatístico e não coluna do dataset\n",
    "dados.describe().select('Summary','_c0', 'state', 'account_length', 'area_code', 'international_plan', 'voice_mail_plan',\n",
    " 'churn').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#análise exploratória - campo Summary é parâmetro estatístico e não coluna do dataset\n",
    "dados.describe().select('Summary',\n",
    " 'number_vmail_messages',\n",
    " 'total_day_minutes',\n",
    " 'total_day_calls',\n",
    " 'total_day_charge',\n",
    " 'churn').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#análise exploratória - campo Summary é parâmetro estatístico e não coluna do dataset\n",
    "dados.describe().select('Summary',\n",
    " 'total_eve_minutes',\n",
    " 'total_eve_calls',\n",
    " 'total_eve_charge',\n",
    " 'churn').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#análise exploratória - campo Summary é parâmetro estatístico e não coluna do dataset\n",
    "dados.describe().select('Summary',\n",
    " 'total_night_minutes',\n",
    " 'total_night_calls',\n",
    " 'total_night_charge',\n",
    " 'churn').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#análise exploratória - campo Summary é parâmetro estatístico e não coluna do dataset\n",
    "dados.describe().select('Summary',\n",
    " 'total_intl_minutes',\n",
    " 'total_intl_calls',\n",
    " 'total_intl_charge',\n",
    " 'churn').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#análise exploratória - campo Summary é parâmetro estatístico e não coluna do dataset\n",
    "dados.describe().select('Summary',\n",
    " 'number_customer_service_calls',\n",
    " 'churn').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pré-processamento - trocaria null, zero ou outliers por NaN caso estes valores atrapalhassem a análise - inicialmente não é o caso\n",
    "#import numpy as np\n",
    "#from pyspark.sql.functions import when\n",
    "#treino =treino.withColumn(\"number_customer_service_calls\",when(raw_data.number_customer_service_calls==null,np.nan).otherwise(raw_data.number_customer_service_calls))\n",
    "#treino =treino.withColumn(\"total_intl_charge\",when(raw_data.total_intl_charge==0,np.nan).otherwise(raw_data.total_intl_charge))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilizaria caso tivesse feito o ajuste anterior\n",
    "#from pyspark.ml.feature import Imputer\n",
    "#imputer=Imputer(inputCols=[\"number_customer_service_calls\",\"total_intl_charge\"],outputCols=[\"number_customer_service_calls\",\"total_intl_charge\"])\n",
    "#model=imputer.fit(treino)\n",
    "#treino = model.transform(treino)\n",
    "#treino.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformando variáveis fator ou string em numéricas\n",
    "dados.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformando variáveis fator ou string em numéricas\n",
    "from pyspark.sql.types import IntegerType\n",
    "dados = dados.na.replace(['yes', 'no'], ['1', '0'], 'international_plan')\n",
    "dados = dados.withColumn('international_plan', dados['international_plan'].cast(IntegerType()))\n",
    "dados = dados.na.replace(['yes', 'no'], ['1', '0'], 'voice_mail_plan')\n",
    "dados = dados.withColumn('voice_mail_plan', dados['voice_mail_plan'].cast(IntegerType()))\n",
    "dados = dados.na.replace(['yes', 'no'], ['1', '0'], 'churn')\n",
    "dados = dados.withColumn('churn', dados['churn'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "dados = dados.withColumn('area_code', substring(dados.area_code, 11,3).alias('area_code'))\n",
    "dados = dados.withColumn('area_code', dados['area_code'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combinando as variáveis em um único vetor\n",
    "cols = dados.columns\n",
    "#removendo variável target e id e state\n",
    "cols.remove(\"churn\") #target\n",
    "cols.remove(\"_c0\") #id\n",
    "cols.remove(\"state\") #string diversas, poderia substituir por nros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=cols,outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados=assembler.transform(dados)\n",
    "dados.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padronizando a escala\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "dados=standardscaler.fit(dados).transform(dados)\n",
    "dados.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split treino e teste\n",
    "treino, teste = dados.randomSplit([0.7, 0.3], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando equilibrio (se tiver muitas ocorrências da mesma target o ml pode aprender mais sobre este)\n",
    "dataset_size=float(treino.select(\"churn\").count())\n",
    "numPositives=treino.select(\"churn\").where('churn == 1').count()\n",
    "per_ones=(float(numPositives)/float(dataset_size))*100\n",
    "numNegatives=float(dataset_size-numPositives)\n",
    "print('The number of ones are {}'.format(numPositives))\n",
    "print('Percentage of ones are {}'.format(per_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajustando o desequilibrio caso exista\n",
    "BalancingRatio= numNegatives/dataset_size\n",
    "print('BalancingRatio = {}'.format(BalancingRatio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "treino=treino.withColumn(\"classWeights\", when(treino.churn == 1,BalancingRatio).otherwise(1-BalancingRatio))\n",
    "treino.select(\"classWeights\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection using chisquareSelector\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "css = ChiSqSelector(featuresCol='Scaled_features',outputCol='Aspect',labelCol='churn',fpr=0.05)\n",
    "treino=css.fit(treino).transform(treino)\n",
    "teste=css.fit(teste).transform(teste)\n",
    "teste.select(\"Aspect\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construindo o modelo de classificação com algoritmo ML regressão logística\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"churn\", featuresCol=\"Aspect\",weightCol=\"classWeights\",maxIter=10)\n",
    "model=lr.fit(treino)\n",
    "predict_train=model.transform(treino)\n",
    "predict_test=model.transform(teste)\n",
    "predict_test.select(\"churn\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avaliação do modelo\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator=BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"churn\")\n",
    "predict_test.select(\"churn\",\"rawPrediction\",\"prediction\",\"probability\").show(5)\n",
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "#obs. meu note não aguentou essa parte...\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.aggregationDepth,[2,5,10])\\\n",
    "    .addGrid(lr.elasticNetParam,[0.0, 0.5, 1.0])\\\n",
    "    .addGrid(lr.fitIntercept,[False, True])\\\n",
    "    .addGrid(lr.maxIter,[10, 100, 1000])\\\n",
    "    .addGrid(lr.regParam,[0.01, 0.5, 2.0]) \\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(treino)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n",
    "predict_train=cvModel.transform(treino)\n",
    "predict_test=cvModel.transform(teste)\n",
    "print(\"The area under ROC for train set after CV  is {}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"The area under ROC for test set after CV  is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
